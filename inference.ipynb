{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-U6eyqTV_J_",
        "outputId": "eaa29319-2f61-47ad-edec-50c6c70a3321"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "import yaml\n",
        "\n",
        "import vgg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y8E9Y5YhWbV_"
      },
      "outputs": [],
      "source": [
        "with open(\"config.yaml\", 'r') as file:\n",
        "    settings = yaml.safe_load(file)\n",
        "\n",
        "# Hyerparameters\n",
        "total_steps = 16\n",
        "img_savepoint = 4\n",
        "learning_rate = settings[\"learning_rate\"]\n",
        "content_weight = settings[\"content_weight\"]\n",
        "style_weight = settings[\"style_weight\"]\n",
        "path = settings[\"path\"]\n",
        "imsize = settings[\"imsize\"]\n",
        "style_images = settings[\"style_images\"]\n",
        "content_image = settings[\"content_image\"]\n",
        "style_loss_type = settings[\"style_loss_type\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NvBgEbDkWi0y"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "loader = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((imsize, imsize)),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def load_image(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device)\n",
        "\n",
        "def load_checkpoint(file_name, optimizer, generated_path=None, device=None):\n",
        "    if not device:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ckpt = torch.load(file_name, map_location=device)\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state'])\n",
        "    print(\"Optimizer's state loaded!\")\n",
        "    generated = None\n",
        "    if generated_path:\n",
        "        generated = load_image(generated_path)\n",
        "    print(\"Generated image loaded!\")\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "W-rYVFLca7Vo"
      },
      "outputs": [],
      "source": [
        "content_img = load_image(f\"pictures/{content_image}\")\n",
        "style_imgs = [load_image(f\"pictures/{img}\") for img in style_images]\n",
        "\n",
        "generated = content_img.clone().requires_grad_(True)\n",
        "optimizer = optim.LBFGS([generated])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly4oSCNlXuUn",
        "outputId": "9e94947f-4ddc-47f1-ef8b-4d41bbd52d70"
      },
      "outputs": [],
      "source": [
        "model = vgg.VGG().to(device).eval()\n",
        "# Do not load previously generated image, we will start from new content image\n",
        "load_checkpoint(\"vgg-19-c2_ckpt_step_finished.pth\", optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "x8yHB2xNbffW",
        "outputId": "e2359069-6816-45c6-b180-db880059d0a6"
      },
      "outputs": [],
      "source": [
        "def covariance_matrix(features):\n",
        "    batch_size, channel, height, width = features.size()\n",
        "    features = features.view(channel, height * width)\n",
        "    mean = features.mean(1, keepdim=True)\n",
        "    cov = (features - mean).mm((features - mean).t()) / (height * width - 1)\n",
        "    return cov\n",
        "\n",
        "def vincent_loss(generated_feature, style_feature):\n",
        "    gen_cov = covariance_matrix(generated_feature)\n",
        "    style_cov = covariance_matrix(style_feature)\n",
        "    return torch.mean((gen_cov - style_cov) ** 2)\n",
        "\n",
        "if style_loss_type.lower() == \"gram\":\n",
        "    print(\"Using GRAM for style loss\")\n",
        "else:\n",
        "    print(\"Using Vincent's for style loss\")\n",
        "\n",
        "for step in range(total_steps):\n",
        "    print(step)\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        generated_features = model(generated)\n",
        "        content_img_features = model(content_img)\n",
        "        style_imgs_features = [model(style_img) for style_img in style_imgs]\n",
        "\n",
        "        style_loss = content_loss = 0\n",
        "\n",
        "        # iterate through all the features for the chosen layers\n",
        "        for gen_feature, cont_feature, style_features in zip(\n",
        "            generated_features, content_img_features, list(map(list, zip(*style_imgs_features)))\n",
        "        ):\n",
        "\n",
        "            # batch_size will just be 1\n",
        "            batch_size, channel, height, width = gen_feature.shape\n",
        "            content_loss += torch.mean((gen_feature - cont_feature) ** 2)\n",
        "\n",
        "            if style_loss_type.lower() == \"gram\":\n",
        "                # Compute Gram Matrix of generated\n",
        "                G = gen_feature.view(channel, height * width).mm(\n",
        "                    gen_feature.view(channel, height * width).t()\n",
        "                )\n",
        "                # Compute Gram Matrix of Style\n",
        "                for style_feature in style_features:\n",
        "                    A = style_feature.view(channel, height * width).mm(\n",
        "                        style_feature.view(channel, height * width).t()\n",
        "                    )\n",
        "                    style_loss += torch.mean((G - A) ** 2)\n",
        "            else:\n",
        "                # Compute Vincent's Loss\n",
        "                for style_feature in style_features:\n",
        "                    style_loss += vincent_loss(gen_feature, style_feature)\n",
        "\n",
        "        style_loss = style_loss / len(style_imgs)\n",
        "\n",
        "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "        if step + 1 % img_savepoint == 0:\n",
        "            print(f\"Style loss: {style_loss}\")\n",
        "            print(f\"Content loss: {content_loss}\")\n",
        "            print(f\"Total weighted loss:{total_loss}\")\n",
        "\n",
        "        total_loss.backward()\n",
        "        return total_loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "\n",
        "display(Image.open(path))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
